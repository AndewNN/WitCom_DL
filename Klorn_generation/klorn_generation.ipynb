{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n",
    "!pip -q install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import lightning as L\n",
    "# from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7406fcdce990>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128 # B: how many independent sequences will we process in parallel?\n",
    "seq_len = 256    # T: what is the maximum context length for predictions?\n",
    "n_embd = 64     # C: text embedding size\n",
    "n_head = 8      # number of heads\n",
    "n_layer = 4     # number of blocks\n",
    "eval_interval = 200\n",
    "max_iters = eval_interval * 20\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "dropout = 0.0\n",
    "\n",
    "assert \"cuda\" in device, \"This experiment requires a GPU to run.\"\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pra-apai-manee-ch1-50.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Quick implementation of character tokenizer\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1100605]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, data, seq_len):\n",
    "    self.data = data\n",
    "    self.seq_len = seq_len\n",
    "  def __len__(self):\n",
    "    return len(self.data)-seq_len\n",
    "  def __getitem__(self, idx):\n",
    "    return self.data[idx:idx+seq_len], self.data[idx+1:idx+seq_len+1]\n",
    "\n",
    "train_dataset = TextDataset(train_data, seq_len)\n",
    "val_dataset = TextDataset(val_data, seq_len)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(seq_len, seq_len)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        tril = self.tril[:T, :T] == 0\n",
    "        k = self.key(x)                              # (B,T,d_k)\n",
    "        q = self.query(x)                            # (B,T,d_k)\n",
    "        v = self.value(x)                            # (B,T,d_k)\n",
    "\n",
    "        # Calculate the attention scores\n",
    "        wei = 1 / (self.head_size**0.5) * q @ k.permute(0, 2, 1) # Dot product of q * k & normalization (B, T, d_k) @ (B, d_k, T) -> (B, T, T)\n",
    "        wei = torch.where(tril, torch.tensor(float('-inf')), wei)                                       # Use masked_fill on tril (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)                     # Apply softmax (B, T, T)\n",
    "        wei = self.dropout(wei)                          # Added dropout\n",
    "        out = wei @ v                                       # (B, T, T) @ (B, T, d_k) -> (B, T, d_k)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads*head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd * 4, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(seq_len, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x)    # (B,T,C)\n",
    "        x = self.ln_f(x)      # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -seq_len:]            # crop idx to the last block_size tokens\n",
    "            logits, loss = self(idx_cond)           # get the predictions\n",
    "            logits = logits[:, -1, :]               # focus only on the last time step - becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=-1)       # apply softmax to get probabilities - (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # sample from the distribution - (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # append sampled index to the running sequence - (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "class TransformerLMModule(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = TransformerLanguageModel()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        xb, yb = batch\n",
    "        # evaluate the loss\n",
    "        logits, loss = self.model(xb, yb)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        xb, yb = val_batch\n",
    "        logits, loss = self.model(xb, yb)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "\n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        if batch_idx % self.trainer.log_every_n_steps == 0:\n",
    "            now = datetime.now()\n",
    "            print(f'{now.strftime(\"%Y-%m-%dT%H:%M:%S\")} Step: {batch_idx}/{self.trainer.max_steps} Train Loss: {metrics[\"train_loss\"]:.4f}', end='')\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        print(f'\\t\\t\\tVal Loss: {metrics[\"val_loss\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.224839 M parameters\n"
     ]
    }
   ],
   "source": [
    "L.pytorch.seed_everything(42)\n",
    "model = TransformerLMModule()\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                     | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model | TransformerLanguageModel | 224 K  | train\n",
      "-----------------------------------------------------------\n",
      "224 K     Trainable params\n",
      "0         Non-trainable params\n",
      "224 K     Total params\n",
      "0.899     Total estimated model params size (MB)\n",
      "218       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cac3de2e05e40a394784775439d5063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/anaconda3/envs/ML10/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/andre/anaconda3/envs/ML10/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 4.4239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/anaconda3/envs/ML10/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a77bf9406c844ae8821c662d208a662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-06T06:41:33 Step: 0/4000 Train Loss: 4.4303"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7663400f69d4217bbdfd58ddc0d9939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 3.0366\n",
      "2025-06-06T06:41:47 Step: 200/4000 Train Loss: 3.0149"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948f1caf7b394b6dae18819e4bc6098b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.8254\n",
      "2025-06-06T06:41:59 Step: 400/4000 Train Loss: 2.7827"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4681ff17e05b4cc4b39611c43e8ec01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.5763\n",
      "2025-06-06T06:42:12 Step: 600/4000 Train Loss: 2.5131"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8bd987290b47b0b23714e522fe8bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.4126\n",
      "2025-06-06T06:42:25 Step: 800/4000 Train Loss: 2.3329"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3842f9d399a74ff0ae071154ec39dc92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.3018\n",
      "2025-06-06T06:42:39 Step: 1000/4000 Train Loss: 2.2350"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e562b0c244e43f3bb8380589b86eeb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.2257\n",
      "2025-06-06T06:42:52 Step: 1200/4000 Train Loss: 2.1418"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c2674f862b47969cefc636b4dae67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.1685\n",
      "2025-06-06T06:43:05 Step: 1400/4000 Train Loss: 2.0850"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcc6a03c1da4d0c9c926e7073784c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.1307\n",
      "2025-06-06T06:43:17 Step: 1600/4000 Train Loss: 2.0622"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f035fb42e05c4fc5a70429f95f326aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.0949\n",
      "2025-06-06T06:43:29 Step: 1800/4000 Train Loss: 1.9696"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff91b765073443da5abd7b31acb78a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.0659\n",
      "2025-06-06T06:43:41 Step: 2000/4000 Train Loss: 1.9732"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341a82602a2d461897da5139fe251ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.0444\n",
      "2025-06-06T06:43:53 Step: 2200/4000 Train Loss: 1.9398"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab7431668d04ee9bdbfc4dcea6b64c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.0267\n",
      "2025-06-06T06:44:05 Step: 2400/4000 Train Loss: 1.9412"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5f159d49a646799137cf8ad918a11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 2.0084\n",
      "2025-06-06T06:44:17 Step: 2600/4000 Train Loss: 1.8829"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04f89e7dfc944fbb530cffc27804642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 1.9962\n",
      "2025-06-06T06:44:30 Step: 2800/4000 Train Loss: 1.8787"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324a4a0998a44694a5065e86d976352d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 1.9747\n",
      "2025-06-06T06:44:42 Step: 3000/4000 Train Loss: 1.8764"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b94a34b6f04476b8bb4c58cd9473edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 1.9682\n",
      "2025-06-06T06:44:55 Step: 3200/4000 Train Loss: 1.8503"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9531a898a3f34887838b2d6fabb70f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 1.9612\n",
      "2025-06-06T06:45:07 Step: 3400/4000 Train Loss: 1.8517"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e64cfa9f6b4408289fc96ff675c1d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 1.9462\n",
      "2025-06-06T06:45:19 Step: 3600/4000 Train Loss: 1.8203"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e884b70bdc41568920bba5ec5adf42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 1.9422\n",
      "2025-06-06T06:45:31 Step: 3800/4000 Train Loss: 1.8020"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6161ee3d19f740b3ab805df51ff5b1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=4000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tVal Loss: 1.9349\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    deterministic=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    logger=False,\n",
    "    max_steps=max_iters,\n",
    "    val_check_interval=eval_interval,\n",
    "    log_every_n_steps=eval_interval,\n",
    "    enable_checkpointing=False,  # Enable checkpointing\n",
    "    limit_val_batches=eval_iters,\n",
    "    callbacks=[]\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "if not os.path.exists('../model'):\n",
    "    os.makedirs('../model')\n",
    "trainer.save_checkpoint('../model/klorn_gen.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "best_model = TransformerLMModule.load_from_checkpoint('../model/klorn_gen.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "๏ อาจารย์ภณสอนเอ็นแอลพี\tส่วนนาความเห็นผู้ใดข้าเคย\n",
      "พร้อมพราหมณ์มันลูกเป็นแปด\tตามปีกผลบล้มไท้พึงให้หาย\n",
      "ให้ผูกถูกเห็นจะมาเถ็ดเตษฐ์\tด้วยหลังรพระการเดินเดือนหลาน\n",
      "แต่เลือบขอมลดแดนนแนบบนไม\tอยู่แล้วชาววิตแดนจนหมาย\n",
      "เหมือนป่วนแปลงไม้หลงหลายไป\tใช้ชวนอื่นน้ำหยอกไปสวรค์เป็นหา\n",
      "นางรำลึกที่ช่วยเครื่องทรง\tเหมือนนางแท่นตื่นภายอางค์\n",
      "พวกใหลมกรายสายเครื่องหรือทำนอากปัญญาณ์\tแย้มยิ้มคู่คิดถึงวาลลดลงใดไห้\n",
      "เสียงถือบรับเสร็จที่ถูกหลีกลีลา\n",
      "เป็นขึ้นบุญรพิโสดเศลิกขยิ้มชมทำ\tกลับตวันภพพฤทภัยไม่ไหว้\n",
      "พวกสูรตูเวทแทงพลางเสนาวรัก\tคู่มความรักอัสแรงไรเลย\n",
      "จงโฉมยงย์ทรงสำหรับแศด\tครั้นฉานแก่คิร่งไปถึงไม่วาย\n",
      "ส่งสารสาเสียทุกเกศมาลิ้นชาว\tว่าจาบอกกับกับขวัญดังสืบง\n",
      "เราลีนางต่างนั้นอยู่บรรเห็น\tอีปกกระสุดพ่อเห็นสู้เส้นย\n",
      "มกายกุฎีร้ายทั้งพรายหน่ายังแคลงคณี\tจงพบปีดังหยั่งหวังทั้งมา\n",
      "เจียบแล้วกรานสายสนายรำภาณ์\tเรียบรรทนานิ้วหมูฉาย\n",
      "วิ่งหนีไม่เนื้อเล่าพี่น้องป่อนผ้อยพ่อสองอน ฯ\n",
      "๏ สินสมุทรสุวรรณรันจี่เอากับประ\tจึงชิดตามถึงหามมาไม่ขาม\n",
      "จึงถามตามแต่ปรางนางแรงกาย\tพีเลี้ยงล้ายล้มลิลค่อยฟัน\n",
      "พงพาทีปีศาธานีป้องเคียง\tอยู่เปล่าโศกเข้าแลนแก้ไม่แนบหนี\n",
      "ที่ลิโลมลามว่าข้าจะคิด\tด้ว\n"
     ]
    }
   ],
   "source": [
    "L.pytorch.seed_everything(42)\n",
    "# generate from the model\n",
    "context = torch.tensor([encode(\"๏ อาจารย์ภณสอนเอ็นแอลพี\t\")], dtype=torch.long, device=device)\n",
    "best_model.model.eval()\n",
    "print(decode(best_model.model.to(device).generate(context, max_new_tokens=1000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
